{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data.dataset import random_split\n",
    "import csv\n",
    "from pytorch_transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "         \"../Robera/\", do_lower_case=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2tokens(strl,tokens,s_index):\n",
    "    i=0\n",
    "    t_index=0\n",
    "    while i<s_index:\n",
    "        if tokens[t_index]!='[UNK]':\n",
    "            i+=len(tokens[t_index].strip('#'))\n",
    "        else:\n",
    "            i+=1\n",
    "        t_index+=1\n",
    "    return t_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "栗XX 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['栗', 'xx', '、']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='公诉机关指控，2013年11月以来，被告人张某某在辽阳市太子河区其租住的平房内多次向栗XX、王XX贩卖毒品。2014年2月在辽纺附近向刘XX贩卖500元冰毒。2014年4月在辽阳市白塔区外环路XX大院门前，张某某向刘XX贩卖500元冰毒。2014年4月，张某某在其租住的平房处向赵XX贩卖麻古15粒。2013年11月，张某某在其租住的平房处向魏XX贩卖500元冰毒。案发后，被告人张某某被公安机关抓获归案。公诉机关认为，被告人张某某的行为已触犯了《中华人民共和国刑法》第三百四十七条之规定，应以贩卖毒品罪追究其刑事责任。'\n",
    "l=tokenizer.tokenize(s)\n",
    "st=42\n",
    "en=st+len(\"栗XX\")\n",
    "e=list2tokens(s,l,en)\n",
    "print(s[st:en],e)\n",
    "l[38:40+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用bertTokenizer切词\n",
    "with open('train.json','rt',encoding='utf-8') as fin1,open('test.json','rt',encoding='utf-8') as fin2,\\\n",
    "open('train.sent','w',newline='',encoding='utf-8') as ftrs,open('dev.sent','w',newline='',encoding='utf-8') as fds,\\\n",
    "open('test.sent','w',newline='',encoding='utf-8') as ftes,open('train.pointer','w',newline='',encoding='utf-8') as ftrp,\\\n",
    "open('dev.pointer','w',newline='',encoding='utf-8') as fdp,open('test.pointer','w',newline='',encoding='utf-8') as ftep:\n",
    "\n",
    "    train_=[]\n",
    "    for line in fin1:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        train_.append(sentence)\n",
    "    train_size=len(train_)\n",
    "    val_size = int(0.25 * len(train_))\n",
    "    train_,val_= random_split(train_, [train_size-val_size,val_size])\n",
    "    train_=list(train_)\n",
    "    val_=list(val_)\n",
    "    \n",
    "    for tr in train_:\n",
    "        sentence_text=tr[\"sentText\"].strip().strip('\"')\n",
    "        ftrs.write(sentence_text+'\\n')\n",
    "        sentence_l=tokenizer.tokenize(sentence_text)\n",
    "               \n",
    "        pointer=[]\n",
    "        for relation in tr['relationMentions']:     \n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            e1s=relation['e1start']\n",
    "            e1start=list2tokens(sentence_text,sentence_l,e1s)\n",
    "            e1e=e1s+len(relation['em1Text'])-1\n",
    "            e1end=list2tokens(sentence_text,sentence_l,e1e)\n",
    "            e2s=relation['e21start']\n",
    "            e2start=list2tokens(sentence_text,sentence_l,e2s)\n",
    "            e2e=e2s+len(relation['em2Text'])-1\n",
    "            e2end=list2tokens(sentence_text,sentence_l,e2e)\n",
    "#             print(sentence_l[e1start: e1end + 1])\n",
    "            if e1end>512 or e2e>512:\n",
    "                continue\n",
    "            e1start=str(e1start)\n",
    "            e1end=str(e1end)\n",
    "            e2start=str(e2start)\n",
    "            e2end=str(e2end)\n",
    "            \n",
    "            pointer.append(' '.join([e1start,e1end,e2start,e2end,re_type]))\n",
    "        \n",
    "        ftrp.write(' | '.join(pointer)+'\\n')\n",
    "           \n",
    "    for vl in val_:\n",
    "        sentence_text=vl[\"sentText\"].strip().strip('\"')\n",
    "        fds.write(sentence_text+'\\n')\n",
    "        sentence_l=tokenizer.tokenize(sentence_text)\n",
    "        \n",
    "        pointer=[]\n",
    "        for relation in vl['relationMentions']:\n",
    "            e1s=relation['e1start']\n",
    "#             print(e1s,len(sentence_text),len(sentence_l))\n",
    "            e1start=list2tokens(sentence_text,sentence_l,e1s)\n",
    "            e1e=e1s+len(relation['em1Text'])-1\n",
    "            e1end=list2tokens(sentence_text,sentence_l,e1e)\n",
    "            e2s=relation['e21start']\n",
    "            e2start=list2tokens(sentence_text,sentence_l,e2s)\n",
    "            e2e=e2s+len(relation['em2Text'])-1\n",
    "            e2end=list2tokens(sentence_text,sentence_l,e2e)\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            if e1end>512 or e2end>512:\n",
    "                continue\n",
    "            e1start=str(e1start)\n",
    "            e1end=str(e1end)\n",
    "            e2start=str(e2start)\n",
    "            e2end=str(e2end)\n",
    "            \n",
    "            pointer.append(' '.join([e1start,e1end,e2start,e2end,re_type]))\n",
    "        fdp.write(' | '.join(pointer)+'\\n')\n",
    "        \n",
    "    test_=[]\n",
    "    for line in fin2:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        test_.append(sentence)\n",
    "    \n",
    "    for te in test_:\n",
    "        sentence_text=te[\"sentText\"].strip().strip('\"')\n",
    "        ftes.write(sentence_text+'\\n')\n",
    "        sentence_l=tokenizer.tokenize(sentence_text)\n",
    "        \n",
    "        pointer=[]\n",
    "        for relation in te['relationMentions']:\n",
    "            e1s=relation['e1start']\n",
    "            e1start=list2tokens(sentence_text,sentence_l,e1s)\n",
    "            e1e=e1s+len(relation['em1Text'])-1\n",
    "            e1end=list2tokens(sentence_text,sentence_l,e1e)\n",
    "            e2s=relation['e21start']\n",
    "            e2start=list2tokens(sentence_text,sentence_l,e2s)\n",
    "            e2e=e2s+len(relation['em2Text'])-1\n",
    "            e2end=list2tokens(sentence_text,sentence_l,e2e)\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            if e1end>512 or e2end>512:\n",
    "                continue\n",
    "            e1start=str(e1start)\n",
    "            e1end=str(e1end)\n",
    "            e2start=str(e2start)\n",
    "            e2end=str(e2end)\n",
    "            \n",
    "            pointer.append(' '.join([e1start,e1end,e2start,e2end,re_type]))\n",
    "        ftep.write(' | '.join(pointer)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json','rt',encoding='utf-8') as fin1,open('test.json','rt',encoding='utf-8') as fin2,\\\n",
    "open('train.tup','w',newline='',encoding='utf-8') as ftrt,open('dev.tup','w',newline='',encoding='utf-8') as fdt,\\\n",
    "open('test.tup','w',newline='',encoding='utf-8') as ftet:\n",
    "#     train_=[]\n",
    "#     for line in fin1:\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue  #结束则跳出循环\n",
    "#         sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "#         train_.append(sentence)\n",
    "#     train_size=len(train_)\n",
    "#     val_size = int(0.25 * len(train_))\n",
    "#     train_,val_= random_split(train_, [train_size-val_size,val_size])\n",
    "#     train_=list(train_)\n",
    "#     val_=list(val_)\n",
    "    \n",
    "    for tr in train_:\n",
    "        tuples=[]\n",
    "        for relation in tr['relationMentions']:\n",
    "            e1=relation['em1Text']\n",
    "            e2=relation['em2Text']\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            tuples.append(' ; '.join([e1,e2,re_type]))\n",
    "        ftrt.write(' | '.join(tuples)+'\\n')\n",
    "           \n",
    "    for vl in val_:\n",
    "        \n",
    "        tuples=[]\n",
    "        for relation in vl['relationMentions']:\n",
    "            e1=relation['em1Text']\n",
    "            e2=relation['em2Text']\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            tuples.append(' ; '.join([e1,e2,re_type]))\n",
    "        fdt.write(' | '.join(tuples)+'\\n')\n",
    "        \n",
    "    test_=[]\n",
    "    for line in fin2:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        test_.append(sentence)\n",
    "    \n",
    "    \n",
    "    for te in test_:\n",
    "        tuples=[]\n",
    "        for relation in te['relationMentions']:\n",
    "            e1=relation['em1Text']\n",
    "            e2=relation['em2Text']\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            tuples.append(' ; '.join([e1,e2,re_type]))\n",
    "        ftet.write(' | '.join(tuples)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用list()切词，以字符为单位\n",
    "with open('train.json','rt',encoding='utf-8') as fin1,open('test.json','rt',encoding='utf-8') as fin2,\\\n",
    "open('train.sent','w',newline='',encoding='utf-8') as ftrs,open('dev.sent','w',newline='',encoding='utf-8') as fds,\\\n",
    "open('test.sent','w',newline='',encoding='utf-8') as ftes,open('train.pointer','w',newline='',encoding='utf-8') as ftrp,\\\n",
    "open('dev.pointer','w',newline='',encoding='utf-8') as fdp,open('test.pointer','w',newline='',encoding='utf-8') as ftep:\n",
    "\n",
    "    train_=[]\n",
    "    for line in fin1:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        train_.append(sentence)\n",
    "    train_size=len(train_)\n",
    "    val_size = int(0.25 * len(train_))\n",
    "    train_,val_= random_split(train_, [train_size-val_size,val_size])\n",
    "    train_=list(train_)\n",
    "    val_=list(val_)\n",
    "    \n",
    "    for tr in train_:\n",
    "        sentence_text=tr[\"sentText\"].strip().strip('\"')\n",
    "        ftrs.write(sentence_text+'\\n')\n",
    "        pointer=[]\n",
    "        for relation in tr['relationMentions']:\n",
    "            e1start=relation['e1start']\n",
    "            e1end=e1start+len(relation['em1Text'])-1\n",
    "            e2start=relation['e21start']\n",
    "            e2end=e2start+len(relation['em2Text'])-1\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            if e1end>512 or e2end>512:\n",
    "                continue\n",
    "            e1start=str(e1start)\n",
    "            e1end=str(e1end)\n",
    "            e2start=str(e2start)\n",
    "            e2end=str(e2end)\n",
    "            \n",
    "            pointer.append(' '.join([e1start,e1end,e2start,e2end,re_type]))\n",
    "        ftrp.write(' | '.join(pointer)+'\\n')\n",
    "           \n",
    "    for vl in val_:\n",
    "        sentence_text=vl[\"sentText\"].strip().strip('\"')\n",
    "        fds.write(sentence_text+'\\n')\n",
    "        pointer=[]\n",
    "        for relation in vl['relationMentions']:\n",
    "            e1start=relation['e1start']\n",
    "            e1end=e1start+len(relation['em1Text'])-1\n",
    "            e2start=relation['e21start']\n",
    "            e2end=e2start+len(relation['em2Text'])-1\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            if e1end>512 or e2end>512:\n",
    "                continue\n",
    "            e1start=str(e1start)\n",
    "            e1end=str(e1end)\n",
    "            e2start=str(e2start)\n",
    "            e2end=str(e2end)\n",
    "            \n",
    "            pointer.append(' '.join([e1start,e1end,e2start,e2end,re_type]))\n",
    "        fdp.write(' | '.join(pointer)+'\\n')\n",
    "        \n",
    "    test_=[]\n",
    "    for line in fin2:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        test_.append(sentence)\n",
    "    \n",
    "    for te in test_:\n",
    "        sentence_text=te[\"sentText\"].strip().strip('\"')\n",
    "        ftes.write(sentence_text+'\\n')\n",
    "        pointer=[]\n",
    "        for relation in te['relationMentions']:\n",
    "            e1start=relation['e1start']\n",
    "            e1end=e1start+len(relation['em1Text'])-1\n",
    "            e2start=relation['e21start']\n",
    "            e2end=e2start+len(relation['em2Text'])-1\n",
    "            re_type=relation['label']\n",
    "            if re_type=='NA':\n",
    "                continue\n",
    "            if e1end>512 or e2end>512:\n",
    "                continue\n",
    "            e1start=str(e1start)\n",
    "            e1end=str(e1end)\n",
    "            e2start=str(e2start)\n",
    "            e2end=str(e2end)\n",
    "            \n",
    "            pointer.append(' '.join([e1start,e1end,e2start,e2end,re_type]))\n",
    "        ftep.write(' | '.join(pointer)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([str(1),str(2),str(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
