{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NewBio/wangzhuoyue/.conda/envs/cyg/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data.dataset import random_split\n",
    "import csv\n",
    "from pytorch_transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "         \"../../backup/Robera/\", do_lower_case=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2tokens(strl,tokens,s_index):\n",
    "    i=0\n",
    "    t_index=0\n",
    "    while i<s_index:\n",
    "        if tokens[t_index]!='[UNK]':\n",
    "            i+=len(tokens[t_index].strip('#'))\n",
    "        else:\n",
    "            i+=1\n",
    "        t_index+=1\n",
    "    return t_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "栗XX 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['栗', 'xx', '、']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='公诉机关指控，2013年11月以来，被告人张某某在辽阳市太子河区其租住的平房内多次向栗XX、王XX贩卖毒品。2014年2月在辽纺附近向刘XX贩卖500元冰毒。2014年4月在辽阳市白塔区外环路XX大院门前，张某某向刘XX贩卖500元冰毒。2014年4月，张某某在其租住的平房处向赵XX贩卖麻古15粒。2013年11月，张某某在其租住的平房处向魏XX贩卖500元冰毒。案发后，被告人张某某被公安机关抓获归案。公诉机关认为，被告人张某某的行为已触犯了《中华人民共和国刑法》第三百四十七条之规定，应以贩卖毒品罪追究其刑事责任。'\n",
    "l=tokenizer.tokenize(s)\n",
    "st=42\n",
    "en=st+len(\"栗XX\")\n",
    "e=list2tokens(s,l,en)\n",
    "print(s[st:en],e)\n",
    "l[38:40+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用bertTokenizer切词\n",
    "test_=[]\n",
    "with open('cail_drug.json','rt',encoding='utf-8') as fin2,\\\n",
    "open('test_cail.sent','w',newline='',encoding='utf-8') as ftes,\\\n",
    "open('test_cail.pointer','w',newline='',encoding='utf-8') as ftep:\n",
    "       \n",
    "    \n",
    "    for line in fin2:\n",
    "        \n",
    "        line = line.strip()\n",
    "        # print(line)\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        try:\n",
    "            sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "            test_.append(sentence)\n",
    "        except:\n",
    "            pass\n",
    "    # print(test_)\n",
    "    test_=test_[-100:]\n",
    "    for te in test_:\n",
    "        sentence_text= \"\".join(te[\"fact\"].strip().strip('\"').split())\n",
    "        # print(sentence_text)\n",
    "        if(len(sentence_text)>510):\n",
    "            sentence_text =sentence_text[:510]\n",
    "        ftes.write(sentence_text+'\\n')\n",
    "        sentence_l=tokenizer.tokenize(sentence_text)\n",
    "        \n",
    "        pointer=[]\n",
    "        pointer.append(' '.join(['0','0','0','0','posess']))\n",
    "        ftep.write(' | '.join(pointer)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_cail.tup','w',newline='',encoding='utf-8') as ftet:\n",
    "            \n",
    "    for te in test_:\n",
    "        tuples=[]\n",
    "        tuples.append(' ; '.join(['e1','e2','posses']))\n",
    "        ftet.write(' | '.join(tuples)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用list()切词，以字符为单位 BIOES\n",
    "with open('test_cail.sent','rt',encoding='utf-8') as fin1,open('test_cail.pointer','rt',encoding='utf-8') as fin2,\\\n",
    "open('test_cail.ner','w',newline='',encoding='utf-8') as ftrs:\n",
    "    count=0\n",
    "#     train_=[]\n",
    "    for line1,line2 in zip(fin1,fin2):\n",
    "        line1 = line1.strip()\n",
    "#         train_.append(line1)\n",
    "        if len(line1)>256:\n",
    "            count+=1\n",
    "        line2=line2.strip().split('|')\n",
    "        if not line1:\n",
    "            continue  #结束则跳出循环\n",
    "        res=['O'for i in range(len(line1))]\n",
    "        \n",
    "        ftrs.write(' '.join(res)+'\\n')\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['尼可待因', '倍它羟基芬太尼', '克醋托啡', '布桂嗪', '地唑辛', '羟基四氢甲基二苯吡喃', '舒芬太尼', '地芬诺酯', '丁丙诺啡', '白色晶体', '丙吡胺', '冰毒', '氯胺酮', '卡西酮', '强痛定', '阿法甲基硫代芬太尼', '伪麻黄碱', '麻果', '白色粉末状毒品', '艾司唑仑', '埃托啡', '盐酸二氢埃托啡', '“麻果”', '安钠咖', '大麻油', '麻黄碱', '氯氮卓', '阿法甲基芬太尼', '甲基苯丙胺晶体', 'k粉', '甲卡西酮', '左啡诺', '乙酰阿法甲基芬太尼', '仲氟代芬太尼', '二甲氧基安非他明', '醋酸酐', '阿片', '二乙基色胺', '油', '辣面', '安那度尔', '芬太尼', 'K粉', '1-苯乙基-4-苯基-4-4哌啶丙盐酸', '3-甲基硫代芬太尼', '度冷丁', '阿芬太尼', '3-甲基芬太尼', '咖啡因', '福尔可定', '“白粉”', '甲基苯丙胺', '二氢埃托啡', '去甲吗啡', '二乙酰吗啡', '凯托米酮', '杜度冷丁', '麻古', '溴西泮等', '非那西汀', '大麻脂', '苯丙胺', '三氯甲烷', '瑞芬太尼', '去甲可待因', '尼美西泮', '鸦片', '大麻烟', '芬纳西泮', '罂粟壳', '乙基吗啡', '甲基苯丙胺片剂', '盐酸二氢埃托啡片', '倍它羟基-3-甲基芬太尼', '尼二可待因', '镇痛新', '布托啡诺', '海洛因', '醋氢可待因', '替利定', '可卡因', '甲基苯丙胺（冰毒）', '乙醚', '麻古丸', '大麻', '喷他佐辛', '基苯丙胺片剂', '氢吗啡醇', '吗啡', '“K粉”', '羟吗啡酮', '亚甲基二氧甲基苯丙胺', '地西泮', '吗啉乙基吗啡', '四氢大麻酚', '神仙水', '美沙酮', '氢可酮', '可待因', '硫代芬太尼', '羟考酮', '地索吗啡', '安眠酮', '哌替啶', '麻黄浸膏粉', '丙氧芬', '半粒麻古', '布莱丙胺', '大麻叶', '二亚甲基双氧安非他明', '麻黄浸膏', '白色晶体状物质', '1-甲基-4-苯基-4-哌啶丙盐酸', '氢吗啡酮', '三唑仑', '双氧可待因']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 使用list()切词，以字符为单位  BIO\n",
    "with open('test_cail.sent','rt',encoding='utf-8') as fin1,open('lexicon.txt','rt',encoding='utf-8') as fin2,\\\n",
    "open('test_cail.lexiconf','w',newline='',encoding='utf-8') as ftrs:\n",
    "    lex=fin2.read().split()\n",
    "    print(lex)\n",
    "    count=0\n",
    "#     train_=[]\n",
    "    for line1 in fin1:\n",
    "        line1 = line1.strip()\n",
    "#         train_.append(line1)\n",
    "        if len(line1)>256:\n",
    "            count+=1\n",
    "       \n",
    "        if not line1:\n",
    "            continue  #结束则跳出循环\n",
    "        res=[]\n",
    "        for l in lex:\n",
    "            f=re.finditer(l,line1)\n",
    "            for x in f:\n",
    "                if x.span()[0]<512 and x.span()[1]<512:\n",
    "                    res+=[str(x.span()[0]),str(x.span()[1])]\n",
    "#                     res[x.span()[0]][x.span()[1]]='1'\n",
    "#                 for i in range(x.span()[0],x.span()[1]):\n",
    "#                     if i<512:\n",
    "#                         res[i+1]='1'\n",
    "        assert len(res)%2==0\n",
    "                \n",
    "        ftrs.write(' '.join(res)+'\\n')\n",
    "            \n",
    "        \n",
    "#         for temp in line2:\n",
    "#             t=temp.split()\n",
    "#             e1s=int(t[0])\n",
    "#             e1e=int(t[1])\n",
    "#             e2s=int(t[2])\n",
    "#             e2e=int(t[3])\n",
    "#             if t[4]=='sell_drugs_to' or t[4]=='provide_shelter_for':\n",
    "#                 res[e1s]='B'\n",
    "#                 res[e2s]='B'\n",
    "#                 for i in range(e1s+1,e1e+1):\n",
    "#                     res[i]='I'\n",
    "#                 for i in range(e2s+1,e2e+1):\n",
    "#                     res[i]='I'\n",
    "#             elif t[4]=='posess' or t[4]=='traffic_in':\n",
    "#                 res[e1s]='B'\n",
    "#                 res[e2s]='B'\n",
    "#                 for i in range(e1s+1,e1e+1):\n",
    "#                     res[i]='I'\n",
    "#                 for i in range(e2s+1,e2e+1):\n",
    "#                     res[i]='I'\n",
    "#         ftrs.write(' '.join(res)+'\\n')\n",
    "#         train_.append(line1)\n",
    "#         print(line1,line2,res)\n",
    "#         break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5682c8dd74534a5b86604ebfdf1a9718b668b6b464067af0570d1cda970d2075"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('cyg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
